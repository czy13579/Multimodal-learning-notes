## VisualGPTä½œç”¨
æ³¨é‡Šæ•°æ®çš„æœ‰é™å¯ç”¨æ€§å¾€å¾€ä¼šé˜»ç¢æœºå™¨å­¦ä¹ åœ¨çŽ°å®žä¸–ç•Œä¸­çš„åº”ç”¨ã€‚ä¸ºäº†é«˜æ•ˆåœ°ä»Žå°‘é‡å¤šæ¨¡æ€æ•°æ®ä¸­å­¦ä¹ ï¼ŒVisualGPTåˆ©ç”¨äº†æ¥è‡ªå¤§åž‹é¢„è®­ç»ƒè¯­è¨€æ¨¡åž‹ï¼ˆPLMï¼‰çš„è¯­è¨€çŸ¥è¯†ï¼Œå¹¶å°†å…¶å¿«é€Ÿè°ƒæ•´åˆ°æ–°çš„å›¾åƒå­—å¹•é¢†åŸŸã€‚è¦æœ‰æ•ˆåˆ©ç”¨é¢„è®­ç»ƒæ¨¡åž‹ï¼Œ**å¹³è¡¡è§†è§‰è¾“å…¥å’Œæ¥è‡ªé¢„è®­ç»ƒçš„å…ˆéªŒè¯­è¨€çŸ¥è¯†**è‡³å…³é‡è¦ã€‚

VisualGPT é‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„è‡ªæ ¡æ­£ç¼–ç å™¨-è§£ç å™¨æ³¨æ„æœºåˆ¶ï¼Œå¯åˆ©ç”¨**å°‘é‡åŸŸå†…å›¾åƒ-æ–‡æœ¬æ•°æ®å¿«é€Ÿè°ƒæ•´ PLM**ã€‚æ‰€æå‡ºçš„è‡ªæ ¡æ­£æ¿€æ´»å•å…ƒå¯äº§ç”Ÿç¨€ç–æ¿€æ´»ï¼Œ**é˜²æ­¢è¯­è¨€çŸ¥è¯†è¢«æ„å¤–è¦†ç›–**ã€‚

## VisualGPTæ¨¡åž‹ç»“æž„
![alt text](image.png)
ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›å°†è§†è§‰ç¼–ç ð¼å’Œè§£ç å™¨çš„å½“å‰çŠ¶æ€ð»ä½œä¸ºè¾“å…¥ã€‚äº¤å‰æ³¨æ„åŠ›å±‚ä»¥ð»ä½œä¸ºæŸ¥è¯¢ï¼Œä»¥ð¼ä½œä¸ºé”®å’Œå€¼æ¥åº”ç”¨æ³¨æ„åŠ›æ“ä½œã€‚ç¼–ç å™¨-è§£ç å™¨çš„å…³æ³¨åº¦ä¸ºï¼š
![alt text](image-2.png)

åœ¨è®¾è®¡ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›æ—¶ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä»”ç»†**å¹³è¡¡ç¼–ç å™¨æä¾›çš„è§†è§‰ä¿¡æ¯å’Œ PLM ä¸­å­˜å‚¨çš„è¯­è¨€çŸ¥è¯†**ã€‚
- åœ¨ç”Ÿæˆ "äºº"ã€"å¡è½¦ "æˆ– "ç‹— "ç­‰è§†è§‰è¯æ—¶ï¼Œæ¨¡åž‹åº”å…³æ³¨è§†è§‰ä¿¡æ¯ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œ
- ç”Ÿæˆé™å®šè¯æˆ–è¿žæŽ¥è¯åªéœ€è¦è¯­è¨€çŸ¥è¯†ã€‚

ç†æƒ³æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›åˆ©ç”¨ PLM æƒé‡ä¸­å­˜å‚¨çš„å¤§é‡è¯­è¨€çŸ¥è¯†ï¼ŒåŒæ—¶åªåœ¨éœ€è¦æ—¶æ‰å‚è€ƒè§†è§‰è¾“å…¥ã€‚ä¸ºäº†å®žçŽ°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€å¯¹ä¸“é—¨çš„**é—¨æŽ§å•å…ƒ**ã€‚

### Self-Resurrecting Activation Unit(è‡ªæ ¡æ­£æ¿€æ´»å•å…ƒ)
![alt text](image-1.png)
**EncDecAttn(ð»,ð¼)å¯ä»¥çœ‹ä½œæ˜¯ç”¨è§†è§‰ä¿¡æ¯ð¼å¯¹è¯­è¨€ä¿¡æ¯ð»è¿›è¡Œç¼–ç **ã€‚åœ¨ VisualGPT ä¸­ï¼Œä½¿ç”¨äº†ä¸¤ä¸ªäº’è¡¥é—¨$B^{vis}$å’Œ$B^{lan}$æ¥æŽ§åˆ¶è¿™ä¸¤ç§æ¨¡å¼ä¹‹é—´çš„å¹³è¡¡ã€‚è¯¥æ¨¡å—çš„è¾“å‡ºä¸º:
![alt text](image-3.png)

![alt text](image-4.png)
![alt text](image-5.png)
![alt text](image-6.png)

SRAU çš„å¦ä¸€ä¸ªä¼˜åŠ¿æ˜¯å®ƒ**èƒ½å¤Ÿæ‘†è„±é›¶è¾“å‡º**ã€‚ä¸€ä¸ªé—¨çš„è¾“å‡ºå¯èƒ½ä¸ºé›¶ï¼Œæ¢¯åº¦ä¸ºé›¶ï¼Œè€Œå¦ä¸€ä¸ªé—¨çš„æ¢¯åº¦ä»ç„¶å¯ç”¨è¿™ç§ä¸å¯¹ç§°æ€§ä½¿å¾—åŸºäºŽæ¢¯åº¦çš„ä¼˜åŒ–å¯ä»¥é€šè¿‡æ”¹å˜å…¶ä»–é—¨æ¥æ”¹å˜é›¶è¾“å‡ºé—¨ã€‚è¯¥é—¨æŽ§å•å…ƒè¢«å‘½åä¸º**è‡ªæ ¡æ­£æ¿€æ´»å•å…ƒ**ã€‚

å®žçŽ°ï¼š
```
## encoder_outputï¼šè§†è§‰ç¼–ç å™¨è¾“å‡º
def forward(self, x, layer_past=None,mask_queries=None,encoder_output=None,mask_encoder=None, mask_self_attention=None, tau = 0):
    threshold = tau ##é˜ˆå€¼ 

    self_attention, present = self.attn(self.ln_1(x), layer_past=layer_past,mask_self_attention=mask_self_attention)
    a = x + self_attention
    a = self.resid_pdrop(a)

    ## åˆ†åˆ«ä½¿ç”¨ä¸‰å±‚è§†è§‰ç¼–ç å™¨çš„è¾“å‡ºä½œä¸ºkeyå’Œvalue
    ## ä½¿ç”¨çš„ViTåªæœ‰3å±‚
    ## è§†è§‰åˆ†æ”¯äº¤å‰æ³¨æ„åŠ›çš„è¾“å‡º
    enc_att1 = self.enc_dec_attn(x=self.ln_1(a), encoder_output=self.ln_1(encoder_output[:, 0]),mask_encoder=mask_encoder)
    
    enc_att2 = self.enc_dec_attn(x=self.ln_1(a), encoder_output=self.ln_1(encoder_output[:, 1]),mask_encoder=mask_encoder)
    
    enc_att3 = self.enc_dec_attn(x=self.ln_1(a), encoder_output=self.ln_1(encoder_output[:, 2]),mask_encoder=mask_encoder)
    
    ## é—¨æŽ§å€¼
    alpha1 = torch.sigmoid(self.fc_alpha1(torch.cat([a, enc_att1], -1)))
    alpha2 = torch.sigmoid(self.fc_alpha2(torch.cat([a, enc_att2], -1)))
    alpha3 = torch.sigmoid(self.fc_alpha3(torch.cat([a, enc_att3], -1)))


    linguistics_alpha1_mask = torch.where(alpha1 > threshold, torch.ones_like(alpha1), torch.zeros_like(alpha1))
    linguistics_alpha2_mask = torch.where(alpha2 > threshold, torch.ones_like(alpha2), torch.zeros_like(alpha2))
    linguistics_alpha3_mask = torch.where(alpha3 > threshold, torch.ones_like(alpha3), torch.zeros_like(alpha3))


    visual_alpha1_mask = torch.where(alpha1 < 1-threshold, torch.ones_like(alpha1), torch.zeros_like(alpha1))
    visual_alpha2_mask = torch.where(alpha2 < 1-threshold, torch.ones_like(alpha2), torch.zeros_like(alpha2))
    visual_alpha3_mask = torch.where(alpha3 < 1-threshold, torch.ones_like(alpha3), torch.zeros_like(alpha3))


    ## åˆå¹¶è§†è§‰åˆ†æ”¯å’Œè¯­è¨€åˆ†æ”¯
    enc_att1 = alpha1* linguistics_alpha1_mask * a + (1-alpha1)* visual_alpha1_mask * enc_att1
    enc_att2 = alpha2* linguistics_alpha2_mask * a + (1-alpha2)* visual_alpha2_mask * enc_att2
    enc_att3 = alpha3* linguistics_alpha3_mask * a + (1-alpha3)* visual_alpha3_mask* enc_att3

    enc_att = (enc_att1 + enc_att2 + enc_att3) / np.sqrt(3)
    a = enc_att * mask_queries

    m = self.mlp(self.ln_2(a))

    encoder_result = a + m

    encoder_result = self.resid_pdrop(encoder_result)

    encoder_result = encoder_result  * mask_queries
    return encoder_result, present
```

