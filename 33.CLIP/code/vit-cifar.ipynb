{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torchvision\nfrom torchvision import transforms","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-10T06:30:59.799613Z","iopub.execute_input":"2024-07-10T06:30:59.799941Z","iopub.status.idle":"2024-07-10T06:31:05.361266Z","shell.execute_reply.started":"2024-07-10T06:30:59.799915Z","shell.execute_reply":"2024-07-10T06:31:05.360240Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"class Args:\n    img_size=224\n    train_batch_size=16\n    test_batch_size=32\n    \nargs=Args()","metadata":{"execution":{"iopub.status.busy":"2024-07-10T06:31:12.582033Z","iopub.execute_input":"2024-07-10T06:31:12.583980Z","iopub.status.idle":"2024-07-10T06:31:12.588688Z","shell.execute_reply.started":"2024-07-10T06:31:12.583947Z","shell.execute_reply":"2024-07-10T06:31:12.587666Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from torchvision import transforms\n## 数据增强\ntransform_train = transforms.Compose([\n        transforms.RandomResizedCrop((args.img_size, args.img_size), scale=(0.05, 1.0)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n    ])\ntransform_test = transforms.Compose([\n    transforms.Resize((args.img_size, args.img_size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n])\ntransforms={\n    'train':transform_train,\n    'test':transform_test\n}","metadata":{"execution":{"iopub.status.busy":"2024-07-10T06:31:16.539713Z","iopub.execute_input":"2024-07-10T06:31:16.540058Z","iopub.status.idle":"2024-07-10T06:31:16.547636Z","shell.execute_reply.started":"2024-07-10T06:31:16.540032Z","shell.execute_reply":"2024-07-10T06:31:16.546559Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"## 加载数据\ntrain_dataset=torchvision.datasets.CIFAR10(\n    root='./',\n    train=True,\n    download=True,\n    transform=transforms['train']\n)\ntest_dataset=torchvision.datasets.CIFAR10(\n    root='./',\n    train=False,\n    download=True,\n    transform=transforms['test']\n)\n\ntrain_dataloader=torch.utils.data.DataLoader(\n    dataset=train_dataset,\n    batch_size=args.train_batch_size,\n    pin_memory=True\n)\n\ntest_dataloader=torch.utils.data.DataLoader(\n    dataset=test_dataset,\n    batch_size=args.test_batch_size,\n    pin_memory=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T06:31:19.534957Z","iopub.execute_input":"2024-07-10T06:31:19.535305Z","iopub.status.idle":"2024-07-10T06:31:25.133549Z","shell.execute_reply.started":"2024-07-10T06:31:19.535277Z","shell.execute_reply":"2024-07-10T06:31:25.132777Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170498071/170498071 [00:02<00:00, 79862523.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./cifar-10-python.tar.gz to ./\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## ViT实现","metadata":{}},{"cell_type":"code","source":"from torch import nn\ndef swish(x):\n    return x*torch.sigmoid(x)\nACT2FN={\n    \"gelu\":nn.functional.gelu,\n    \"relu\":nn.functional.relu,\n    \"swish\":swish\n}\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T03:19:30.068294Z","iopub.execute_input":"2024-07-10T03:19:30.069124Z","iopub.status.idle":"2024-07-10T03:19:30.073851Z","shell.execute_reply.started":"2024-07-10T03:19:30.069089Z","shell.execute_reply":"2024-07-10T03:19:30.072883Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"## embedding\nclass Embedding(nn.Module):\n    def __init__(self, img_size=32,patch_size=4, in_c=3, embed_dim=768,dropout_rate=0.1):\n        super().__init__()\n        img_size=(img_size,img_size)\n        patch_size=(patch_size,patch_size)\n        self.grid_size=(img_size[0]//patch_size[0],img_size[1]//patch_size[1])\n        self.num_patch=self.grid_size[0]*self.grid_size[1]\n        self.proj=nn.Conv2d(in_c,embed_dim,kernel_size=patch_size,stride=patch_size)\n        self.position_embeddings=nn.Parameter(torch.zeros(1,1,embed_dim))\n        self.cls_token=nn.Parameter(torch.zeros(1,1,embed_dim))\n        self.dropout=nn.Dropout(dropout_rate)\n    def forward(self,x):\n        B=x.shape[0]\n        cls_tokens=self.cls_token.expand(B,-1,-1)\n        x=self.proj(x)\n        x=x.flatten(2)\n        x=x.transpose(-1,-2)\n        x=torch.cat((cls_tokens,x),dim=1)\n        embeddings=x+self.position_embeddings\n        embeddings=self.dropout(embeddings)\n        return embeddings","metadata":{"execution":{"iopub.status.busy":"2024-07-10T05:43:54.830832Z","iopub.execute_input":"2024-07-10T05:43:54.831200Z","iopub.status.idle":"2024-07-10T05:43:54.840947Z","shell.execute_reply.started":"2024-07-10T05:43:54.831171Z","shell.execute_reply":"2024-07-10T05:43:54.840034Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self,dim,num_heads=12,qkv_bias=None,attn_drop_rate=0.,proj_drop_rate=0.):\n        super().__init__()\n        self.num_heads=num_heads\n        self.head_dim=dim//num_heads\n        new_dim=self.head_dim*self.num_heads\n        self.new_dim=new_dim\n        #self.scale=qk_scale or head_dim**(-0.5)\n        self.q=nn.Linear(dim,new_dim)\n        self.k=nn.Linear(dim,new_dim)\n        self.v=nn.Linear(dim,new_dim)\n        \n        self.attn_drop=nn.Dropout(attn_drop_rate)\n        self.o_proj=nn.Linear(new_dim,dim)\n        self.proj_drop=nn.Dropout(proj_drop_rate)\n        \n        self.softmax=nn.Softmax(dim=-1)\n    def transpose_for_scores(self,x):\n        new_x_shape=x.shape[:-1]+(self.num_heads,self.head_dim)\n        x=x.view(*new_x_shape)\n        return x.permute(0,2,1,3) ##( B,head_nums,length,head_dim)\n    def forward(self,x):\n        B,N,C=x.shape\n        q=self.q(x)\n        k=self.k(x)\n        v=self.v(x)\n        \n        q=self.transpose_for_scores(q)\n        k=self.transpose_for_scores(k)\n        v=self.transpose_for_scores(v)\n        \n        attention_scores=torch.matmul(q,k.transpose(-2,-1))\n        attention_scores=attention_scores*(self.head_dim**(-0.5))\n        attention_prods=self.softmax(attention_scores)## (B,head_nums,length,length)\n        attention_prods=self.attn_drop(attention_prods)\n        \n        new_values=torch.matmul(attention_prods,v)## (B,head_nums,length,head_dim)\n        new_values=new_values.permute(0,2,1,3).contiguous()\n        new_shape=new_values.shape[:-2]+(self.new_dim,)\n        new_values=new_values.view(*new_shape)## (B,length,new_dim)\n        \n        output=self.o_proj(new_values)\n        output=self.proj_drop(output)\n        return output\n        \n        \nclass MLP(nn.Module):\n    def __init__(self,hidden_dim,mlp_dim,dropout_rate=0.1):\n        super().__init__()\n        self.fc1=nn.Linear(hidden_dim,mlp_dim)\n        self.fc2=nn.Linear(mlp_dim,hidden_dim)\n        self.act=ACT2FN['gelu']\n        self.dropout=nn.Dropout(dropout_rate)\n        self._init_weights()\n\n    def _init_weights(self):\n        nn.init.xavier_uniform_(self.fc1.weight)\n        nn.init.xavier_uniform_(self.fc2.weight)\n        nn.init.normal_(self.fc1.bias, std=1e-6)\n        nn.init.normal_(self.fc2.bias, std=1e-6)\n\n    def forward(self,x):\n        x=self.fc1(x)\n        x=self.act(x)\n        x=self.dropout(x)\n        x=self.fc2(x)\n        x=self.dropout(x)\n        return x\n\nclass Block(nn.Module):\n    def __init__(self,hidden_dim):\n        super().__init__()\n        self.layer_norm_attn=nn.LayerNorm(hidden_dim,eps=1e-6)\n        self.layer_norm_ffn=nn.LayerNorm(hidden_dim,eps=1e-6)\n        self.ffn=MLP(hidden_dim,hidden_dim*4,0.2)\n        self.attn=Attention(hidden_dim)\n    def forward(self,x):\n        h=x\n        x=self.layer_norm_attn(x)\n        x=self.attn(x)\n        x=x+h\n        \n        h=x\n        x=self.layer_norm_ffn(x)\n        x=self.ffn(x)\n        x=x+h\n        return x\n    \nclass Encoder(nn.Module):\n    def __init__(self,hidden_dim,num_layer):\n        super().__init__()\n        self.embedding=Embedding()\n        self.blocks=nn.ModuleList()\n        self.layer_norm=nn.LayerNorm(hidden_dim,eps=1e-6)\n        for i in range(num_layer):\n            self.blocks.append(Block(hidden_dim))\n    def forward(self,x):\n        x=self.embedding(x)\n        for layer in self.blocks:\n            x=layer(x)\n        x=self.layer_norm(x)\n        return x\n\nclass ViT(nn.Module):\n    def __init__(self,hidden_dim,num_layer,num_classes):\n        super().__init__()\n        self.encoder=Encoder(hidden_dim,num_layer)\n        self.head=nn.Linear(hidden_dim,num_classes)\n    def forward(self,x):\n        x=self.encoder(x)\n        logits=self.head(x[:,0])\n        return logits\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2024-07-10T05:43:56.989625Z","iopub.execute_input":"2024-07-10T05:43:56.989995Z","iopub.status.idle":"2024-07-10T05:43:57.015251Z","shell.execute_reply.started":"2024-07-10T05:43:56.989963Z","shell.execute_reply":"2024-07-10T05:43:57.014332Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"markdown","source":"## 从零开始训练","metadata":{}},{"cell_type":"code","source":"from transformers import get_scheduler\nhidden_dim=768\nnum_layer=12\nnum_classes=10\ndevice=torch.device('cuda')\nmodel=ViT(hidden_dim,num_layer,num_classes).to(device)\ndevice_ids = [0, 1]\nmodel= torch.nn.DataParallel(model, device_ids=device_ids)\noptimizer=torch.optim.AdamW(model.parameters(),lr=8e-4)\n#optimizer = nn.DataParallel(optimizer, device_ids=device_ids)\nnum_warmup_steps=100\nnum_training_steps=2000\nlr_scheduler = get_scheduler(\n    name=\"cosine\", optimizer=optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps\n)\n\nloss_fn = nn.CrossEntropyLoss()\nprint(f'参数量:{sum(m.numel() for m in model.parameters())/1e6}M')","metadata":{"execution":{"iopub.status.busy":"2024-07-10T06:25:09.250996Z","iopub.execute_input":"2024-07-10T06:25:09.251569Z","iopub.status.idle":"2024-07-10T06:25:10.439156Z","shell.execute_reply.started":"2024-07-10T06:25:09.251536Z","shell.execute_reply":"2024-07-10T06:25:10.438163Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"## 验证\nfrom tqdm import tqdm\n@torch.no_grad()\ndef eval_model():\n    model.eval()\n    total=0\n    acc=0\n    for batch,labels in tqdm(test_dataloader):\n        batch=batch.to(device)\n        labels=labels.to(device)\n        logits= model(batch).view(-1,num_classes)\n        pred=torch.argmax(logits,dim=-1)\n        labels=labels.view(-1)\n        acc+=torch.sum(pred==labels)\n        total+=pred.shape[0]\n    print('acc:',acc/total)\neval_model()","metadata":{"execution":{"iopub.status.busy":"2024-07-10T05:24:14.170991Z","iopub.execute_input":"2024-07-10T05:24:14.171392Z","iopub.status.idle":"2024-07-10T05:24:35.094221Z","shell.execute_reply.started":"2024-07-10T05:24:14.171362Z","shell.execute_reply":"2024-07-10T05:24:35.093198Z"},"trusted":true},"execution_count":88,"outputs":[{"name":"stderr","text":"100%|██████████| 157/157 [00:20<00:00,  7.51it/s]","output_type":"stream"},{"name":"stdout","text":"acc: tensor(0.0142, device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"## 训练\nfrom tqdm import tqdm\nnum_epochs=5\nfor epoch in range(num_epochs):\n    model.train()\n    for batch,labels in tqdm(train_dataloader):\n        batch=batch.to(device)\n        labels=labels.to(device)\n        logits= model(batch)\n        loss=loss_fn(logits.view(-1,num_classes),labels.view(-1))\n        # 反向传播和参数更新\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n    eval_model()\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 加载预训练模型","metadata":{}},{"cell_type":"code","source":"model=torchvision.models.vit_b_16(pretrained=True)\nnum_classes=10\nmodel.heads=torch.nn.Linear(768,num_classes)\ndevice=torch.device('cuda')\nmodel=model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T06:46:50.373125Z","iopub.execute_input":"2024-07-10T06:46:50.373890Z","iopub.status.idle":"2024-07-10T06:46:51.905686Z","shell.execute_reply.started":"2024-07-10T06:46:50.373842Z","shell.execute_reply":"2024-07-10T06:46:51.904867Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"device=torch.device('cuda')\noptimizer=torch.optim.AdamW(model.parameters(),lr=2e-5)\nloss_fn = torch.nn.CrossEntropyLoss()\nprint(f'参数量:{sum(m.numel() for m in model.parameters())/1e6}M')","metadata":{"execution":{"iopub.status.busy":"2024-07-10T06:47:01.178609Z","iopub.execute_input":"2024-07-10T06:47:01.178973Z","iopub.status.idle":"2024-07-10T06:47:01.188611Z","shell.execute_reply.started":"2024-07-10T06:47:01.178943Z","shell.execute_reply":"2024-07-10T06:47:01.187708Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"参数量:85.806346M\n","output_type":"stream"}]},{"cell_type":"code","source":"## 验证\nfrom tqdm import tqdm\n@torch.no_grad()\ndef eval_model():\n    model.eval()\n    total=0\n    acc=0\n    for batch,labels in tqdm(test_dataloader):\n        batch=batch.to(device)\n        labels=labels.to(device)\n        logits= model(batch).view(-1,num_classes)\n        pred=torch.argmax(logits,dim=-1)\n        labels=labels.view(-1)\n        acc+=torch.sum(pred==labels)\n        total+=pred.shape[0]\n    print('acc:',acc/total)\n## eval_model()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## 训练\nfrom tqdm import tqdm\nnum_epochs=1\nfor epoch in range(num_epochs):\n    model.train()\n    i=0\n    for batch,labels in tqdm(train_dataloader):\n        batch=batch.to(device)\n        labels=labels.to(device)\n        logits= model(batch)\n        loss=loss_fn(logits.view(-1,num_classes),labels.view(-1))\n        # 反向传播和参数更新\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        i+=1\n        if i>100:\n            break\n    eval_model()\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T06:47:45.617255Z","iopub.execute_input":"2024-07-10T06:47:45.617653Z","iopub.status.idle":"2024-07-10T06:50:42.704382Z","shell.execute_reply.started":"2024-07-10T06:47:45.617622Z","shell.execute_reply":"2024-07-10T06:50:42.703500Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"  3%|▎         | 100/3125 [01:00<30:41,  1.64it/s]\n100%|██████████| 313/313 [01:56<00:00,  2.70it/s]","output_type":"stream"},{"name":"stdout","text":"acc: tensor(0.9051, device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"all_models = torchvision.models.list_models()\nall_models","metadata":{"execution":{"iopub.status.busy":"2024-07-10T06:38:55.700140Z","iopub.execute_input":"2024-07-10T06:38:55.701055Z","iopub.status.idle":"2024-07-10T06:38:55.710620Z","shell.execute_reply.started":"2024-07-10T06:38:55.701015Z","shell.execute_reply":"2024-07-10T06:38:55.709523Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"['alexnet',\n 'convnext_base',\n 'convnext_large',\n 'convnext_small',\n 'convnext_tiny',\n 'deeplabv3_mobilenet_v3_large',\n 'deeplabv3_resnet101',\n 'deeplabv3_resnet50',\n 'densenet121',\n 'densenet161',\n 'densenet169',\n 'densenet201',\n 'efficientnet_b0',\n 'efficientnet_b1',\n 'efficientnet_b2',\n 'efficientnet_b3',\n 'efficientnet_b4',\n 'efficientnet_b5',\n 'efficientnet_b6',\n 'efficientnet_b7',\n 'efficientnet_v2_l',\n 'efficientnet_v2_m',\n 'efficientnet_v2_s',\n 'fasterrcnn_mobilenet_v3_large_320_fpn',\n 'fasterrcnn_mobilenet_v3_large_fpn',\n 'fasterrcnn_resnet50_fpn',\n 'fasterrcnn_resnet50_fpn_v2',\n 'fcn_resnet101',\n 'fcn_resnet50',\n 'fcos_resnet50_fpn',\n 'googlenet',\n 'inception_v3',\n 'keypointrcnn_resnet50_fpn',\n 'lraspp_mobilenet_v3_large',\n 'maskrcnn_resnet50_fpn',\n 'maskrcnn_resnet50_fpn_v2',\n 'maxvit_t',\n 'mc3_18',\n 'mnasnet0_5',\n 'mnasnet0_75',\n 'mnasnet1_0',\n 'mnasnet1_3',\n 'mobilenet_v2',\n 'mobilenet_v3_large',\n 'mobilenet_v3_small',\n 'mvit_v1_b',\n 'mvit_v2_s',\n 'quantized_googlenet',\n 'quantized_inception_v3',\n 'quantized_mobilenet_v2',\n 'quantized_mobilenet_v3_large',\n 'quantized_resnet18',\n 'quantized_resnet50',\n 'quantized_resnext101_32x8d',\n 'quantized_resnext101_64x4d',\n 'quantized_shufflenet_v2_x0_5',\n 'quantized_shufflenet_v2_x1_0',\n 'quantized_shufflenet_v2_x1_5',\n 'quantized_shufflenet_v2_x2_0',\n 'r2plus1d_18',\n 'r3d_18',\n 'raft_large',\n 'raft_small',\n 'regnet_x_16gf',\n 'regnet_x_1_6gf',\n 'regnet_x_32gf',\n 'regnet_x_3_2gf',\n 'regnet_x_400mf',\n 'regnet_x_800mf',\n 'regnet_x_8gf',\n 'regnet_y_128gf',\n 'regnet_y_16gf',\n 'regnet_y_1_6gf',\n 'regnet_y_32gf',\n 'regnet_y_3_2gf',\n 'regnet_y_400mf',\n 'regnet_y_800mf',\n 'regnet_y_8gf',\n 'resnet101',\n 'resnet152',\n 'resnet18',\n 'resnet34',\n 'resnet50',\n 'resnext101_32x8d',\n 'resnext101_64x4d',\n 'resnext50_32x4d',\n 'retinanet_resnet50_fpn',\n 'retinanet_resnet50_fpn_v2',\n 's3d',\n 'shufflenet_v2_x0_5',\n 'shufflenet_v2_x1_0',\n 'shufflenet_v2_x1_5',\n 'shufflenet_v2_x2_0',\n 'squeezenet1_0',\n 'squeezenet1_1',\n 'ssd300_vgg16',\n 'ssdlite320_mobilenet_v3_large',\n 'swin3d_b',\n 'swin3d_s',\n 'swin3d_t',\n 'swin_b',\n 'swin_s',\n 'swin_t',\n 'swin_v2_b',\n 'swin_v2_s',\n 'swin_v2_t',\n 'vgg11',\n 'vgg11_bn',\n 'vgg13',\n 'vgg13_bn',\n 'vgg16',\n 'vgg16_bn',\n 'vgg19',\n 'vgg19_bn',\n 'vit_b_16',\n 'vit_b_32',\n 'vit_h_14',\n 'vit_l_16',\n 'vit_l_32',\n 'wide_resnet101_2',\n 'wide_resnet50_2']"},"metadata":{}}]}]}