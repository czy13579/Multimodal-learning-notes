## ALIGN 作用
ALIGN(A Large-scale ImaGe and Noisy-text embedding)：意为大规模图像和噪声文本嵌入。
数据收集，处理，标注过程往往需要消耗大量的人力物力，因此高质量的数据是非常昂贵的。标注质量高的数据集往往数据量不够丰富，数据量丰富的原始数据往往带有大量的噪声，自监督模型如何解决数据量与数据质量之间的矛盾呢？
ALIGN利用一个**未经过复杂预处理的大规模数据集**进行预训练，在zero-shot图片分类，跨模态文本检索，图片检索，文本-图片联合检索等任务都取得了很好的效果。

## ALIGN与CLIP对比

### 数据集和训练方式
- ALIGN模型：
数据集：ALIGN模型利用了超过10亿的图像文本对的噪声数据集，这些数据集**没有进行数据过滤或后处理步骤，保留了原始数据中图像文本对的自然分布**。
训练方式：ALIGN模型使用对比学习损失，通过一个简单的双编码器结构来学习对齐图像和文本对的视觉和语言表示。这种方法避免了繁重的数据管理和注释工作，只需要最小的基于频率的清理。
- CLIP模型：
数据集：CLIP模型的数据集则通常涉及复杂的数据收集过程和预处理，如MS-COCO、Visual Genome等高质量但规模相对较小的人群标记数据集。此外，CLIP还构建了一个名为WebImageText（WIT）的新数据集，包含4亿对图像和文本数据，这些数据来自互联网上的各种公开可用资源。
训练方式：CLIP模型同样采用对比学习的方法，但其在数据集的选择和构建上更为精细，旨在覆盖更广泛的视觉概念集。

### 编码器架构
- ALIGN模型：
图像编码器：ALIGN模型使用**EfficientNet作为图像编码器**，该编码器具有高效的卷积神经网络结构，能够在保持高准确性的同时**减少计算量和参数量**。
文本编码器：ALIGN模型使用**BERT作为文本编码器**。
- CLIP模型：
图像编码器：CLIP模型的图像编码器有两种主要架构，一种是基于ResNet50的改进版本，另一种是基于Vision Transformer（ViT）的架构。这两种架构都旨在从图像中提取有效的特征表示。
文本编码器：CLIP模型的文本编码器同样基于Transformer架构，但进行了特定的修改以适应CLIP的预训练任务。

### 目标和任务
- ALIGN模型：
主要目标是利用大规模噪声图像文本数据来**扩展视觉和视觉语言表示学习**，实现跨模态检索和zero-shot视觉分类等任务。
- CLIP模型：
CLIP模型旨在通过对比学习将文本和图像编码进行对齐，从而能够执行广泛的任务，包括OCR、地理定位、动作识别等。CLIP模型特别适用于零样本学习任务，即模型可以在不看到新的图像或文本训练示例的情况下进行预测。


## 资源
- [论文](https://arxiv.org/abs/2102.05918)
- [code](https://github.com/kakaobrain/coyo-align/tree/main)
- [论文解读](https://blog.csdn.net/weixin_51697828/article/details/122040130)
